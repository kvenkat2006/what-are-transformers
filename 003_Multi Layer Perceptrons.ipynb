{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\" > Deep Learning Basics </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider Multi-Layer Perceptrons (MLPs) which are relevant to our discussion. Here is a depiction of an MLP. \n",
    "\n",
    "<img src=\"./images/FullyConnectedNN_allWeightsDrawn.png\" width=\"400\" height=\"400\" alt=\"An MLP\">\n",
    "\n",
    "Now, to look deeper into how this is put to work, let's consider the case of a 3-layer network consisting of an input layer, a hidden layer and an output layer. For simplicity, let the hidden and output layers have just one neuron each, as shown below. \n",
    "\n",
    "\n",
    "<img src=\"./images/ASingleArtificialNeuron.png\" width=\"400\" height=\"400\" alt=\"A Single Neuron\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (In the above diagram, read underscores as subscript operations. i.e x_4 as $x_4$)\n",
    "\n",
    " - Let $x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "x_5\n",
    "\\end{pmatrix} \\in \\mathbb{R^5}\n",
    "$ be the inputs.\n",
    "\n",
    " -  $ W = \n",
    "\\begin{pmatrix}\n",
    "w_1 & w_2 & w_3 & w_4 & w_5 \n",
    "\\end{pmatrix}\n",
    "$, \n",
    "a 1x5 matrix of weights. \n",
    "\n",
    "The output from the hidden layer is give by:\n",
    "$$ h = \\sigma(Wx + b) $$\n",
    "where \n",
    "* $b$ a bias term\n",
    "* $\\sigma$ is an activation function which can be one of these:\n",
    "  *  $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $  (Sigmoid function, used in output layer for binary classification)\n",
    "  * $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $ (Hyperbolic Tangent, tanh. Used in hidden layers)\n",
    "  * $ \\text{ReLU}(x) = \\max(0, x) $  (ReLU, Rectified Linear Unit, widely used in hidden layers)\n",
    "  * $ \\text{Leaky ReLU}(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ \\alpha x & \\text{if } x < 0 \\end {cases} $ (A variant of ReLU that allows a small gradient when ð‘¥ is negative.)\n",
    "  * Softmax:  $ \\sigma(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $ (Used in the output layer of multi-class classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us add more nodes to hidden and output layers and see how to feed forward computations look. \n",
    "\n",
    "<img src=\"./images/FullyConnectedNN_SomeWeightsDrawn.png\" width=\"400\" height=\"400\" alt=\"A Single Neuron\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above network:\n",
    "$x =\n",
    "\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "x_5\n",
    "\\end{pmatrix} \\in \\mathbb{R^5}\n",
    "$ are the inputs.\n",
    "\n",
    "Only some of the weights associated with $h_1$ are shown. The complete weights matrix is: \n",
    " -  $ W = \n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} & w_{15} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} & w_{25} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34} & w_{35} \\\\\n",
    "w_{41} & w_{42} & w_{43} & w_{44} & w_{45} \\\\\n",
    "w_{51} & w_{52} & w_{53} & w_{54} & w_{55} \\\\\n",
    "w_{61} & w_{62} & w_{63} & w_{64} & w_{65} \\\\\n",
    "\\end{pmatrix}\n",
    "$, \n",
    "a 6x5 matrix of weights. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the hidden layer is give by:\n",
    "\n",
    "* $ h =\\begin{pmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "h_4 \\\\\n",
    "h_5 \\\\\n",
    "h_6\n",
    "\\end{pmatrix} =  \\sigma(Wx + b) $\n",
    "where \n",
    " $b =\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3 \\\\\n",
    "b_4 \\\\\n",
    "b_5 \\\\\n",
    "b_6\n",
    "\\end{pmatrix} \\in \\mathbb{R^6}$ is the bias term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $ Wx + b = z$ \n",
    "\n",
    "$ \\begin{pmatrix}\n",
    "w_{11} & w_{12} & w_{13} & w_{14} & w_{15} \\\\\n",
    "w_{21} & w_{22} & w_{23} & w_{24} & w_{25} \\\\\n",
    "w_{31} & w_{32} & w_{33} & w_{34} & w_{35} \\\\\n",
    "w_{41} & w_{42} & w_{43} & w_{44} & w_{45} \\\\\n",
    "w_{51} & w_{52} & w_{53} & w_{54} & w_{55} \\\\\n",
    "w_{61} & w_{62} & w_{63} & w_{64} & w_{65} \\\\\n",
    "\\end{pmatrix}$\n",
    "$\\begin{pmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "x_4 \\\\\n",
    "x_5\n",
    "\\end{pmatrix} = $\n",
    "$\\begin{pmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3 \\\\\n",
    "z_4 \\\\\n",
    "z_5 \\\\\n",
    "z_6\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Then, \n",
    "$ h =\\begin{pmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3 \\\\\n",
    "h_4 \\\\\n",
    "h_5 \\\\\n",
    "h_6\n",
    "\\end{pmatrix} =  \\sigma(Wx + b) =  $\n",
    "$\\sigma \\begin{pmatrix}\n",
    "z_1 \\\\\n",
    "z_2 \\\\\n",
    "z_3 \\\\\n",
    "z_4 \\\\\n",
    "z_5 \\\\\n",
    "z_6\n",
    "\\end{pmatrix} = $\n",
    "$ \\begin{pmatrix}\n",
    "\\sigma(z_1) \\\\\n",
    "\\sigma(z_2) \\\\\n",
    "\\sigma(z_3) \\\\\n",
    "\\sigma(z_4) \\\\\n",
    "\\sigma(z_5) \\\\\n",
    "\\sigma(z_6)\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the above computation provides output emitted by the hidden layer in the above example. \n",
    "\n",
    "The outputs coming out of final layer are computed similarly,  with weights matrices $V$ and a set of biases $b^{\\prime}$ replacing $W$ and $b$ above. \n",
    "\n",
    "Deep Learning networks typically have many such layers (and hence deep). When the network is *deep* linear transformation, followed by addition of a bias term, followed by application of activation function($\\sigma$) can model any kind of function if the right weights and biases are in place. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Let us generalize this for any layer  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x^{(l-1)}$ be input to layer $l$ (output from layer $l-1$). Forward pass gets calculated as follows:\n",
    "\n",
    "- $ \\mathbf{z}^{(l)} = \\mathbf{W}^{(l)} \\mathbf{x}^{(l-1)} + \\mathbf{b}^{(l)} $\n",
    "where \n",
    "  - $\\mathbf{W}^{(l)}$ : weights matrix of shape $(n_l,n_{l-1})$\n",
    "  - $x^{l-1}$ : input vector of shape $(n_{l-1},1)$\n",
    "  - $b^(l)$ : Bias vector of shape $(n_l,1)$\n",
    "  - $z^{(l)}$ : Output of linear transformation. Shape $(n_l,1)$\n",
    "- Apply Activation function to introduce non-linearity: $ \\mathbf{a}^{(l)} = f(\\mathbf{z}^{(l)}) $\n",
    "    - $f$ can be one of the following functions:\n",
    "        - $RelU$: $RelU(z) = max(0,z)$\n",
    "\n",
    "        - Sigmoid: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "        - Tanh: $tanh(z) = \\frac{e^z-e^{-z}}{e^z+e^{-z}}$\n",
    "    - $a^{(l)}$ is the activated output of shape $(n_l,1)$ emitted by layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Training a Model </span>\n",
    "\n",
    "#### Model Parameters\n",
    "Weights and biases we saw above are model parameters\n",
    "\n",
    "#### Hyper parameters\n",
    "The number of layers in the network (depth of the network), the number of neurons in each layer are hyper parameters. There are other hyper parameters such as Learning Rate, Batch Size and Epochs that we will see soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Model amounts to learning weights and biases that will make the model do the right kind of inferences. What kind of inferences are we talking about? \n",
    "The inferences are about what the model is trained to do. Some examples are:\n",
    "- Given an email, the goal is to classify if it is a spam or not. This is a binary classification. The output layer will need to output one number that indicates the probability of the email being a spam\n",
    "- Predicting the next word in a sentence: In this case, the output would be a probability distribution over a set of tokens.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model happens by backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Backpropagation when loss function is Mean Squared Error </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it simple, let us look at the loss when one data point flows through the network:\n",
    "\n",
    "Now, let us see how this gets applied in learning model parameters. (Read [003_Multi Layer Perceptrons.](<./003_Multi Layer Perceptrons.ipynb>) to get the context)\n",
    "\n",
    "Consider a neural network that has only one hidden layer:\n",
    "\n",
    "Input: $x$\n",
    "\n",
    "Hidden Layer: $a^{(1)} = \\sigma(W^{(1)}x + b^{(1)}) $\n",
    "\n",
    "Output Layer: $y = \\sigma(W^{(2)}a^{(1)} + b^{(2)}) $\n",
    "\n",
    "Loss Function: $L = (y-\\hat{y})^{2} $ where $\\hat{y}$ is the true label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, $$ \\frac{\\partial L}{\\partial y} = 2(y - \\hat{y} ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> Gradients at Output Layer </span>\n",
    "\n",
    "We first need to propogate the error back through the output layer:\n",
    "$$\\frac{\\partial L}{\\partial W^{(2)}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W^{(2)}} = 2(y - \\hat{y}) \\cdot \\sigma'(W{(2)}a^{(1)} + b^{(2)}) \\cdot a^{(1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, for the biased in the output layer:\n",
    "$$\\frac{\\partial L}{\\partial b^{(2)}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial b^{(2)}} = 2(y - \\hat{y}) \\cdot \\sigma'(W{(2)}a^{(1)} + b^{(2)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> Gradients at Hidden Layer </span>\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a^{(1)}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial a^{(1)}} = 2(y - \\hat{y}) \\cdot \\sigma'(W^{(2)}a^{(1)} + b^{(2)}) \\cdot W^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial W^{(1)}} = \\frac{\\partial L}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial W^{(1)}} = \\left[ 2(y - \\hat{y}) \\cdot \\sigma'(W^{(2)}a^{(1)} + b^{(2)}) \\cdot W^{(2)} \\right] \\cdot \\sigma'(W^{(1)}x + b^{(1)}) \\cdot x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial b^{(1)}} = \\frac{\\partial L}{\\partial a^{(1)}} \\cdot \\frac{\\partial a^{(1)}}{\\partial b^{(1)}} = \\left[ 2(y - \\hat{y}) \\cdot \\sigma'(W^{(2)}a^{(1)} + b^{(2)}) \\cdot W^{(2)} \\right] \\cdot \\sigma'(W^{(1)}x + b^{(1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:orange\"> Back Propagation </span>\n",
    "\n",
    "Gradients calculated as above are used to update weights and biases. Here is a summary of steps:\n",
    "* *Forward Pass* Compute the outputs of the network\n",
    "* *Calculate Loss* This is the difference between the value computed by the network and the actual value it should have computed. We showed the loss over a data point. This needs to be summed over all the datapoints for a batch. \n",
    "* *Calculate gradients* As above\n",
    "* *Update Weights and biases:*\n",
    "\n",
    "    * $$W^{(1)} \\rightarrow W^{(1)} -\\eta \\frac{\\partial L}{\\partial W^{(1)}} $$\n",
    "    * $$b^{(1)} \\rightarrow b^{(1)} -\\eta \\frac{\\partial L}{\\partial b^{(1)}} $$\n",
    "    * $$W^{(2)} \\rightarrow W^{(2)} -\\eta \\frac{\\partial L}{\\partial W^{(2)}} $$\n",
    "    * $$b^{(2)} \\rightarrow b^{(2)} -\\eta \\frac{\\partial L}{\\partial b^{(2)}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Backpropagation when loss function is Cross Entropy </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Backpropagation when loss function is Binary Cross Entropy </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\"> Backpropagation when loss function is Categorical Cross Entropy </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
