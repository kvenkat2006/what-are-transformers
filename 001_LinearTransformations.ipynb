{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Linear Algebra\n",
    "\n",
    "To understand how data input into a Neural Network gets transformed, we will need the definition of a Linear Transformation from a vectorspace into another. \n",
    "\n",
    "Let $\\mathbb{R}$ be the set of real numbers. Our discussion will be limited to vector spaces $\\mathbb{R^n}$  over $\\mathbb{R}$; it may be easy to follow the contents below if you apply them to vectorspaces of $\\mathbb{R^2}$ or $\\mathbb{R^3}$ over  $\\mathbb{R}$.\n",
    "\n",
    "#### *Definition: Vector Space* \n",
    "($V$, $\\mathbb{R}$, $+$, $.$) where $V$ is a set and $\\mathbb{R}$ is the set of real numbers, is said to be a vector space with a vector addition operation $+$' and scalar multiplication '$.$', if it satisfies the following axioms: \n",
    "<br>(In the axioms stated below, let us write $a.v$ as $av$ when $a\\in\\mathbb{R}$ and $v \\in V$ )\n",
    "- $u, v \\in V \\implies$ &nbsp;  $u+v$ $\\in$ $V$ (Closure under addition)\n",
    "- $u+v$ = $v+u$ &emsp; $\\forall u,v \\in V$ (Commutativity of addition)\n",
    "- $u + (v+w) = (u+v)+w$ &emsp; $\\forall u,v,w \\in V$ (Associativity of addition)\n",
    "- $\\exists$ a zero vector $0 \\in V$  such that $v+0=v$    $\\forall v \\in V$ (Existense of zero vector)\n",
    "- $\\forall v \\in V$  $\\exists$ an additive inverse, $-v$ such that $v + (-v) = 0$  (Existense of additive inverse)\n",
    "- $a \\in \\mathbb{R}, v \\in V$  $\\implies$  $av \\in V$  (Closure under scalar multiplication)\n",
    "- $a, b \\in \\mathbb{R}, v \\in V$  $\\implies$  $a(bv) = (ab)v$  (Associativity of scalar multiplication)\n",
    "- $a(u+v) = au + av$ $\\forall a \\in \\mathbb{R}, u,v \\in V$ (Distributivity of vector addition )\n",
    "- $(a+b)v = av + bv =$ $\\forall a,b \\in \\mathbb{R}, v \\in V$ (Distributivity of scalar addition)\n",
    "- $1v = v$ &emsp; $\\forall v \\in V$ (Multiplicative identity)\n",
    "\n",
    "\n",
    "\n",
    "$T$ is said to be a linear transformation from \n",
    "\n",
    "To know more, consider reading a textbook on Linear Algebra at an undergrad level. Some suggestions are are below:\n",
    "- https://open.umn.edu/opentextbooks/textbooks/5?form=MG0AV3\n",
    "- https://understandinglinearalgebra.org/home.html?form=MG0AV3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples of Vector spaces:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ($\\mathbb{R}^2$, $\\mathbb{R}$, $+$, $.$) is a vector space. Here the set of vectors is the set of all ordered pairs of real numbers, often visualized as points in a 2D plane. This vector space is of dimension 2. \n",
    "\n",
    "- ($\\mathbb{R}^3$, $\\mathbb{R}$, $+$, $.$) is the vector space of all ordered triples of real numbers, often visualized as points in a 3D plane. This vector space is of dimension 3.\n",
    "\n",
    "- ($\\mathbb{R}^n$, $\\mathbb{R}$, $+$, $.$) is a vector space of dimension $n$.  Going forward, let us write ($\\mathbb{R}^n$, $\\mathbb{R}$, $+$, $.$)  as $\\mathbb{R}^n$\n",
    "\n",
    "- In [**Attention is All You Need**](https://arxiv.org/abs/1706.03762) paper, tokens in the vocabulary are represented by vectors in the 512 dimensional space $\\mathbb{R}^{512}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Transformations\n",
    "\n",
    "A Linear Transformation is a function from a vector space $V$ to another vector space $W$ that preserves vector addition and scalar multiplication. \n",
    "\n",
    "#### Definition: Linear Transformation\n",
    "A function $T : V \\to W$ is a linear transformation if:\n",
    "- $\\forall u,v \\in V$,  &emsp; $ T(u + v) = T(u) + T(v) $\n",
    "- For any $v \\in V$ and scalar $c \\in \\mathbb{R}$, $T(cv) = cT(v)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "\n",
    "We will be interested in linear transformations from $\\mathbb{R^m}$ to $\\mathbb{R^n}$ for some $m$ and $n$\n",
    "\n",
    "Let us represent vectors in $\\mathbb{R^n}$ as column matrices of dimension nx1\n",
    "\n",
    "\n",
    "$v =\n",
    "\\begin{pmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "... \\\\\n",
    "v_n\n",
    "\\end{pmatrix} \\in \\mathbb{R^n}\n",
    "$\n",
    "\n",
    "Let $T : \\mathbb{R^n} \\to \\mathbb{R^m}$ be defined as follows:  $$T(v) = Av \\in \\mathbb{R^m}$$\n",
    "where \n",
    "$$ A = \\begin{pmatrix}\n",
    "a_{11} & a_{12} & a_{13} & ... & a_{1n}\\\\\n",
    "a_{21} & a_{22} & a_{23} & ... & a_{2n}\\\\\n",
    ".\\\\.\\\\. \\\\\n",
    "a_{m1} & a_{m2} & a_{m3} & ... & a_{mn}\n",
    "\\end{pmatrix} \\in \\mathbb{R^n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Geometrically, rotations, reflections, scaling and shearing are all linear transformations. \n",
    "\n",
    "- Projections followed by any of the above transformations are also linear transformations\n",
    "\n",
    "- In neural networks, weights form the entries of matrices associated with linear transformations. Computations at each layer comprise of a linear transformation,followed by an addition of a bias term, and that is followed by an application of softmax function.  See [003_Multi Layer Perceptrons.](<./003_Multi Layer Perceptrons.ipynb>). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
